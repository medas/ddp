vowelTreeError <- testError(vowelTree, vowel.test, vowel.test$y)
print(vowelTreeError)
#SVN
vowelSVM <- svm(y ~ . , data=vowel.train)
print("Test error svm:")
vowelSVMError <- testError(vowelSVM, vowel.test, vowel.test$y )
print(vowelSVMError)
print("What is the error rate when the two methods agree on a prediction?")
sum((predict(vowelTree, vowel.test)==predict(vowelSVM, vowel.test)) & (predict(vowelTree, vowel.test)!=vowel.test$y) & (predict(vowelSVM, vowel.test)!=vowel.test$y))/sum(predict(vowelTree, vowel.test)==predict(vowelSVM, vowel.test))
install.packages("randomForest")
library(ElemStatLearn)
library(randomForest)
library(e1071)
library(medley)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
testError <- function(pred, data, outcome){
sum(predict(pred, data)!= outcome)/length(outcome)
}
#Random forest
vowelTree <- randomForest(y ~ . , data=vowel.train, prox = TRUE)
print("Test error random forest:")
vowelTreeError <- testError(vowelTree, vowel.test, vowel.test$y)
print(vowelTreeError)
#SVN
vowelSVM <- svm(y ~ . , data=vowel.train)
print("Test error svm:")
vowelSVMError <- testError(vowelSVM, vowel.test, vowel.test$y )
print(vowelSVMError)
print("What is the error rate when the two methods agree on a prediction?")
sum((predict(vowelTree, vowel.test)==predict(vowelSVM, vowel.test)) & (predict(vowelTree, vowel.test)!=vowel.test$y) & (predict(vowelSVM, vowel.test)!=vowel.test$y))/sum(predict(vowelTree, vowel.test)==predict(vowelSVM, vowel.test))
install.packages("e1071")
install.packages("e1071")
library(ElemStatLearn)
library(randomForest)
library(e1071)
library(medley)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
testError <- function(pred, data, outcome){
sum(predict(pred, data)!= outcome)/length(outcome)
}
#Random forest
vowelTree <- randomForest(y ~ . , data=vowel.train, prox = TRUE)
print("Test error random forest:")
vowelTreeError <- testError(vowelTree, vowel.test, vowel.test$y)
print(vowelTreeError)
#SVN
vowelSVM <- svm(y ~ . , data=vowel.train)
print("Test error svm:")
vowelSVMError <- testError(vowelSVM, vowel.test, vowel.test$y )
print(vowelSVMError)
print("What is the error rate when the two methods agree on a prediction?")
sum((predict(vowelTree, vowel.test)==predict(vowelSVM, vowel.test)) & (predict(vowelTree, vowel.test)!=vowel.test$y) & (predict(vowelSVM, vowel.test)!=vowel.test$y))/sum(predict(vowelTree, vowel.test)==predict(vowelSVM, vowel.test))
library(ElemStatLearn)
library(randomForest)
library(e1071)
library(medley)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
testError <- function(pred, data, outcome){
sum(predict(pred, data)!= outcome)/length(outcome)
}
#Random forest
vowelTree <- randomForest(y ~ . , data=vowel.train, prox = TRUE)
print("Test error random forest:")
vowelTreeError <- testError(vowelTree, vowel.test, vowel.test$y)
print(vowelTreeError)
#SVN
vowelSVM <- svm(y ~ . , data=vowel.train)
print("Test error svm:")
vowelSVMError <- testError(vowelSVM, vowel.test, vowel.test$y )
print(vowelSVMError)
print("What is the error rate when the two methods agree on a prediction?")
sum((predict(vowelTree, vowel.test)==predict(vowelSVM, vowel.test)) & (predict(vowelTree, vowel.test)!=vowel.test$y) & (predict(vowelSVM, vowel.test)!=vowel.test$y))/sum(predict(vowelTree, vowel.test)==predict(vowelSVM, vowel.test))
library("caret", lib.loc="~/R/win-library/3.1")
varImp(vowelSVM)
varImp(vowel)
varImp()
library(ElemStatLearn)
library(randomForest)
library(e1071)
library(medley)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
testError <- function(pred, data, outcome){
sum(predict(pred, data)!= outcome)/length(outcome)
}
#Random forest
vowelTree <- randomForest(y ~ . , data=vowel.train, prox = TRUE)
print("Test error random forest:")
vowelTreeError <- testError(vowelTree, vowel.test, vowel.test$y)
print(vowelTreeError)
#SVN
vowelSVM <- svm(y ~ . , data=vowel.train)
print("Test error svm:")
vowelSVMError <- testError(vowelSVM, vowel.test, vowel.test$y )
print(vowelSVMError)
print("What is the error rate when the two methods agree on a prediction?")
sum((predict(vowelTree, vowel.test)==predict(vowelSVM, vowel.test)) & (predict(vowelTree, vowel.test)!=vowel.test$y) & (predict(vowelSVM, vowel.test)!=vowel.test$y))/sum(predict(vowelTree, vowel.test)==predict(vowelSVM, vowel.test))
varImp(vowelTree$forest)
varImp(vowelTree)
install.packages("knitr")
library("xtable", lib.loc="~/R/win-library/3.1")
?swiss
?lm
install.packages("swirl")
library(swirl)
swirl()
rgp1()
rgp2()
head(swiss)
mdl <- lm(Fertility ~ ., swiss)
vfi(mdl)
vif(mdl)
mdl2 <- lm(Fertility ~ . - Examition, swiss)
mdl2 <- lm(Fertility ~ . - Examination, swiss)
vif(mdl2)
x1c <- simbias()
apply(xi1, 1, mean)
apply(xi1c, 1, mean)
apply(x1c, 1, mean)
fit1 <- lm(Fertility ~ Agriculture, swiss)
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, swiss)
anova(fit1, fit3)
deviance(fit3)
d <- deviance(fit3)/43
n <- deviance(fit1) - deviance(fit3)
n <- (deviance(fit1) - deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail = FALSE)
shapiro.test(fit2$residuals)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
ravendata
ravenData
mdl <- glm(ravenWinNum ~ ravenScore, family, ravenData)
mdl <- glm(ravenWinNum ~ ravenScore, binomial, ravenData)
lodds <- predict(mdl, data.frame(ravenScore=c(0, 3,6)))
exp(lodds)/(1 + exp(lodds))
summary(mdl)
confint(mdl)
exp(confint(mdl))
anova(mdl)
qchsq(0.95, 1)
qchisq(0.95, 1)
var(rpois(1000, 50))
nxt()
View(hits)
class(hits, [,'date'])
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,]
lambda <- mdl$fitted.values[704]
qpois(.95, lambda)
mdl2 <- glm(visits ~ date, poisson, hits)
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits, offset = log(visits
| + 1))
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits, offset = log(visits + 1))
mdl2$fitted.values[704]
qpois(.95, mdl2$fitted.values[704])
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
# predict test
predict1 <- predict(fit1, newdata = vowel.test)
predict2 <- predict(fit2, newdata = vowel.test)
# combine predictions
DF_combined <- data.frame(predict1, predict2, y = vowel.test$y)
fit_combined <- train(y ~ ., data = DF_combined, method = "gam")
predict3 <- predict(fit_combined, newdata = vowel.test)
# confusion matrixes
c1 <- confusionMatrix(predict1, vowel.test$y)
c2 <- confusionMatrix(predict2, vowel.test$y)
c3 <- confusionMatrix(predict3, DF_combined$y)
c1
c2
c3
library(ElemStatLearn)
library(dplyr)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret
# package.
vowel.train <- vowel.train %>%
mutate(y = as.factor(y))
vowel.test <- vowel.test %>%
mutate(y = as.factor(y))
set.seed(33833)
rfModel <- train(y ~ ., method = "rf", data = vowel.train)
gbmModel <- train(y ~ ., method = "gbm", data = vowel.train)
# What are the accuracies for the two approaches on the test data set? What is
# the accuracy among the test set samples where the two methods agree?
rfPred <- predict(rfModel, newdata = vowel.test)
mean(rfPred == vowel.test$y)
gbmPred <- predict(gbmModel, newdata = vowel.test)
mean(gbmPred == vowel.test$y)
agreePred <- rfPred == gbmPred
mean(rfPred[agreePred] == vowel.test$y[agreePred])
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Set the seed to 62433 and predict diagnosis with all the other variables using
# a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis
# ("lda") model. Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set? Is it better or worse than
# each of the individual predictions?
set.seed(62433)
rfModel <- train(diagnosis ~ ., method = "rf", data = training)
gbmModel <- train(diagnosis ~ ., method = "gbm", data = training,
verbose = FALSE)
ldaModel <- train(diagnosis ~ ., method = "lda", data = training)
rfPred <- predict(rfModel, newdata = testing)
mean(rfPred == testing$diagnosis)
gbmPred <- predict(gbmModel, newdata = testing)
mean(gbmPred == testing$diagnosis)
ldaPred <- predict(ldaModel, newdata = testing)
mean(ldaPred == testing$diagnosis)
predDF <- data.frame(rfPred, gbmPred, ldaPred, diagnosis = testing$diagnosis)
combModel <- train(diagnosis ~ ., method = "rf", data = predDF)
combPred <- predict(combModel, newdata = testing)
mean(combPred == testing$diagnosis)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Set the seed to 62433 and predict diagnosis with all the other variables using
# a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis
# ("lda") model. Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set? Is it better or worse than
# each of the individual predictions?
set.seed(62433)
# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")
# predict test
predict1 <- predict(fit1, newdata = testing)
predict2 <- predict(fit2, newdata = testing)
predict3 <- predict(fit3, newdata = testing)
# combine predictions
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)
# confusion matrixes
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)
print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
# Make a histogram and confirm the SuperPlasticizer variable is skewed. Normally
# you might use the log transform to try to make the data more symmetric. Why
# would that be a poor choice for this variable?
ggplot(training, aes(x = log(Superplasticizer+1))) +
geom_histogram()
ggplot(training, aes(x = Superplasticizer)) +
geom_histogram()
fileAddress <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
dir.create("data")
download.file(fileAddress, "data/gaData.csv", method = "curl")
# Using the commands:
library(lubridate)  # For year() function below
dat = read.csv("data/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
fileAddress <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
gwd()
getwd()
download.file(fileAddress, "data/gaData.csv", method = "curl")
library(lubridate)  # For year() function below
dat = read.csv("data/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
set.seed(233)
lassoModel <- train(CompressiveStrength ~ ., method = "lasso", data = training)
plot.enet
x <- training %>%
dplyr::select(-CompressiveStrength) %>%
as.matrix()
y <- training %>%
dplyr::select(CompressiveStrength) %>%
as.matrix()
lassoModel <- enet(x = x, y = y)
plot.enet(lassoModel$finalModel, xvar = "penalty")
dev.off()
??enet
??plot.enet
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 233 and fit a lasso model to predict Compressive Strength.
# Which variable is the last coefficient to be set to zero as the penalty increases?
# (Hint: it may be useful to look up ?plot.enet).
library(caret)
set.seed(233)
fit <- train(CompressiveStrength ~ ., data = training, method = "lasso")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 233 and fit a lasso model to predict Compressive Strength.
# Which variable is the last coefficient to be set to zero as the penalty increases?
# (Hint: it may be useful to look up ?plot.enet).
library(caret)
set.seed(233)
fit <- train(CompressiveStrength ~ ., data = training, method = "lasso")
# Since we are interested in the shrinkage of coefficients as the penalty(lambda) increases, "
# penalty" looks promising for an xvar argument value.
plot.enet(fit$finalModel, xvar = "penalty", use.color = TRUE)
set.seed(233)
lassoModel <- train(CompressiveStrength ~ ., method = "lasso", data = training)
plot.enet
x <- training %>%
dplyr::select(-CompressiveStrength) %>%
as.matrix()
y <- training %>%
dplyr::select(CompressiveStrength) %>%
as.matrix()
lassoModel <- enet(x = x, y = y)
plot.enet(lassoModel$finalModel, xvar = "penalty")
dev.off()
set.seed(233)
lassoModel <- train(CompressiveStrength ~ ., method = "lasso", data = training)
plot.enet
x <- training %>%
dplyr::select(-CompressiveStrength) %>%
as.matrix()
y <- training %>%
dplyr::select(CompressiveStrength) %>%
as.matrix()
lassoModel <- enet(x = x, y = y)
plot.enet(lassoModel$finalModel, xvar = "penalty")
dev.off()
dev.off()
dev.off()
dev.off()
set.seed(233)
lassoModel <- train(CompressiveStrength ~ ., method = "lasso", data = training)
plot.enet
x <- training %>%
dplyr::select(-CompressiveStrength) %>%
as.matrix()
y <- training %>%
dplyr::select(CompressiveStrength) %>%
as.matrix()
lassoModel <- enet(x = x, y = y)
plot.enet(lassoModel$finalModel, xvar = "penalty")
226/235
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 325 and fit a support vector machine using the e1071 package
# to predict Compressive Strength using the default settings. Predict on the
# testing set. What is the RMSE?
set.seed(325)
library(e1071)
x <- training %>%
dplyr::select(-CompressiveStrength)
y <- training %>%
dplyr::select(CompressiveStrength)
svmModel <- svm(x = x, y = y)
svmPred <- predict(svmModel, testing %>% dplyr::select(-CompressiveStrength))
rmse <- sqrt(mean((svmPred - testing$CompressiveStrength)^2, na.rm = TRUE))
rmse
set.seed(62433)
rfModel <- train(diagnosis ~ ., method = "rf", data = training)
gbmModel <- train(diagnosis ~ ., method = "gbm", data = training,
verbose = FALSE)
ldaModel <- train(diagnosis ~ ., method = "lda", data = training)
rfPred <- predict(rfModel, newdata = testing)
mean(rfPred == testing$diagnosis)
gbmPred <- predict(gbmModel, newdata = testing)
mean(gbmPred == testing$diagnosis)
ldaPred <- predict(ldaModel, newdata = testing)
mean(ldaPred == testing$diagnosis)
predDF <- data.frame(rfPred, gbmPred, ldaPred, diagnosis = testing$diagnosis)
combModel <- train(diagnosis ~ ., method = "rf", data = predDF)
combPred <- predict(combModel, newdata = testing)
mean(combPred == testing$diagnosis)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Set the seed to 62433 and predict diagnosis with all the other variables using
# a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis
# ("lda") model. Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set? Is it better or worse than
# each of the individual predictions?
set.seed(62433)
rfModel <- train(diagnosis ~ ., method = "rf", data = training)
gbmModel <- train(diagnosis ~ ., method = "gbm", data = training,
verbose = FALSE)
ldaModel <- train(diagnosis ~ ., method = "lda", data = training)
rfPred <- predict(rfModel, newdata = testing)
mean(rfPred == testing$diagnosis)
gbmPred <- predict(gbmModel, newdata = testing)
mean(gbmPred == testing$diagnosis)
ldaPred <- predict(ldaModel, newdata = testing)
mean(ldaPred == testing$diagnosis)
predDF <- data.frame(rfPred, gbmPred, ldaPred, diagnosis = testing$diagnosis)
combModel <- train(diagnosis ~ ., method = "rf", data = predDF)
combPred <- predict(combModel, newdata = testing)
mean(combPred == testing$diagnosis)
?colSums
?mean
?colSums
?dgamma
?show
?lm
?predict
?dgamma
?lm
?dgamma
??predict
library("rCharts", lib.loc="~/R/win-library/3.1")
ucscDb <- dbConnect(MySQL(),user="genome",
host="genome-mysql.cse.ucsc.edu")
library("DBI", lib.loc="~/R/win-library/3.1")
ucscDb <- dbConnect(MySQL(),user="genome", host="genome-mysql.cse.ucsc.edu")
ucscDb <- dbConnect(MySQL(),user="genome", host="genome-mysql.cse.ucsc.edu")
library("DBI", lib.loc="~/R/win-library/3.1")
ucscDb <- dbConnect(MySQL(),user="genome", host="genome-mysql.cse.ucsc.edu")
R.home()
install.packages('RMySQL',type='source')
ucscDb <- dbConnect(MySQL(),user="genome", host="genome-mysql.cse.ucsc.edu")
library("DBI", lib.loc="~/R/win-library/3.1")
Sys.getenv()
Sys.getenv('MYSQL_HOME')
install.packages('RMySQL',type='source')
install.packages('RMySQL',type='source')
install.packages('RMySQL',type='source')
ucscDb <- dbConnect(MySQL(),user="genome", host="genome-mysql.cse.ucsc.edu")
library("DBI", lib.loc="~/R/win-library/3.1")
ucscDb <- dbConnect(MySQL(),user="genome", host="genome-mysql.cse.ucsc.edu")
library("RMySQL", lib.loc="~/R/win-library/3.1")
ucscDb <- dbConnect(MySQL(),user="genome", host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb, "show databases;")
dbDisconnect(ucscDb)
result
mergedData2 <- merge(reviews, solutions, by.x = "solution_id", by.y = "id", all = TRUE)
shiny::runApp('ddp-cp/app')
shiny::runApp()
shiny::runApp()
